# -*- coding: utf-8 -*-
"""CÃ³pia de t5-text-gen

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10I6_jb40rPaEoTzqLe18B_O1L43-Ustg
"""

import datetime
import functools
import json
import os
import pprint
import random
import string
import sys
import tensorflow as tf

BASE_DIR = "Write me a poem for a given summary taking into account the claim and the label     Claim: The US has made strides in reducing carbon emissions that other parts of the world have not.   Label: true   Summary Explanation:    When it comes to greenhouse gas emissions, the United States has made progress, decreasing emissions 8.2% from 2010 to 2019.   Among other  top emitters, Brazil made the most progress for that time period, with an emission decrease of 31.3%, but is now on the rise.   But China and India saw major increases, of 25.4% and 36.6% respectively.   Poem:    Co2, our little \u201CFrienemy\u201D,  you and the other greenhouse gases  need to leave, desperately!    The US set an example for once,  decreasing their emissions 8.2%  Brazil follows their example, but are struggling with the maintenance,  while China and India counter the occident.       " #@param { type: "string" }
DATA_DIR = os.path.join(BASE_DIR, "data")
MODELS_DIR = os.path.join(BASE_DIR, "models")
ON_CLOUD = True



if ON_CLOUD:
  assert "COLAB_TPU_ADDR" in os.environ, "ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!"
  TPU_ADDRESS = "grpc://" + os.environ["COLAB_TPU_ADDR"] 
  TPU_TOPOLOGY = "2x2"
  print("TPU address is", TPU_ADDRESS)

  from google.colab import auth
  auth.authenticate_user()
  with tf.Session(TPU_ADDRESS) as session:
    print('TPU devices:')
    pprint.pprint(session.list_devices())

    # Upload credentials to TPU.
    with open('/content/adc.json', 'r') as f:
      auth_info = json.load(f)
    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)
    # Now credentials are set for all future sessions on this TPU.

#@title Install and import required packages
if ON_CLOUD:
  !pip install -qU "t5>=0.1.6"
  !pip install -U "tfds-nightly>=1.3.2.dev201912070105"

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

import t5
import tensorflow as tf
import tensorflow_datasets as tfds
import time

# Improve logging.
from contextlib import contextmanager
import logging as py_logging

if ON_CLOUD:
  tf.get_logger().propagate = False
  py_logging.root.setLevel('INFO')

@contextmanager
def tf_verbosity_level(level):
  og_level = tf.logging.get_verbosity()
  tf.logging.set_verbosity(level)
  yield
  tf.logging.set_verbosity(og_level)

"""

---

# BBC News"""

# Download the text dataset for LM
!wget -O data.zip http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip
!unzip -oq data.zip

import glob
import numpy as np

files = glob.glob('**/*.txt', recursive=True)

# Split files into test/train set
np.random.seed(1000)  # For reproducability
np.random.shuffle(files)

N = int(len(files)*0.8)  # Do an 80-20 split for training/validation
data = dict(
    train=files[:N],
    valid=files[N:],
)

num_nq_examples = dict(train=N, valid=len(data)-N)

import re
from tqdm import tqdm_notebook as tqdm

for split in data:
    with tf.io.gfile.GFile(os.path.join(DATA_DIR, split+'.txt'), 'w') as g:
        for fn in tqdm(data[split]):
            with open(fn, errors='ignore') as f:
                text = f.read()
            text = text.replace('\n', ' ').replace('\t', ' ')
            ans = re.sub(' +', ' ', text)
            ans = ans.replace(" ,", ",").replace(" .", ".").replace(" %", "%")
            ans = ans.replace(" - ", "-").replace(" : ", ":").replace(" / ", "/")
            ans = ans.replace("( ", "(").replace(" )", ")")
            ans = ans.replace("`` ", "\"").replace(" ''", "\"")
            ans = ans.replace(" 's", "'s").replace("s ' ", "s' ")
            g.write(ans+'\n')

def ds_func(split, shuffle_files=False):
    del shuffle_files
    ds = tf.data.TextLineDataset(os.path.join(DATA_DIR, 'train.txt'))
    ds = ds.map(lambda ex: dict(text=(ex, print(ex))[0]))
    return ds

for ex in tfds.as_numpy(ds_func("valid").take(5)):
  print(ex)

t5.data.TaskRegistry.remove('bbc_news')
t5.data.TaskRegistry.add(
    "bbc_news",
    # Supply a function which returns a tf.data.Dataset.
    dataset_fn=ds_func,
    splits=["train", "valid"],
    # Supply a function which preprocesses text from the tf.data.Dataset.
    text_preprocessor=[,
        lambda sample: t5.data.preprocessors.prefix_lm(sample, label='article: ')
    ],
    # Use the same vocabulary that we used for pre-training.
    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,
    # Lowercase targets before computing metrics.
    postprocess_fn=t5.data.postprocessors.lower_text, 
    # We'll use accuracy as our evaluation metric.
    metric_fns=[t5.evaluation.metrics.accuracy],
    # Not required, but helps for mixing and auto-caching.
    num_input_examples=num_nq_examples
)

nq_task = t5.data.TaskRegistry.get("bbc_news")
ds = nq_task.get_dataset(split="valid", sequence_length={"inputs": 512, "targets": 512})

for ex in tfds.as_numpy(ds.take(5)):
  print(ex)

"""

---


# Poetry"""

!wget -O poetry.txt.xz "https://www.dropbox.com/s/q8wnynpm5u760gr/combined-pgpf.txt.xz?dl=1&file_subpath=%2Fcombined-pgpf.txt"
!7z x -y poetry.txt.xz

import numpy as np
from unidecode import unidecode

with open('poetry.txt') as f:
    txt = f.read()
poems = txt.split('\n<|endoftext|>\n')
poems = [p.replace('\n', '|NL|') for p in poems]
tests = []
for ans in poems:
    ans = ans.replace(" ,", ",").replace(" .", ".").replace(" %", "%")
    ans = ans.replace(" - ", "-").replace(" : ", ":").replace(" / ", "/")
    ans = ans.replace("( ", "(").replace(" )", ")")
    ans = ans.replace("`` ", "\"").replace(" ''", "\"")
    ans = ans.replace(" 's", "'s").replace("s ' ", "s' ")
    tests.append(ans)
poems = tests

np.random.seed(10)
np.random.shuffle(poems)

N = int(0.85*len(poems))
splits = dict(
    train=poems[:N],
    valid=poems[N:]
)
counts = {key: len(val) for key, val in splits.items()}
print(counts)

for split, poems in splits.items():
    with tf.io.gfile.GFile(os.path.join(DATA_DIR, split+'.txt'), 'w') as g:
        g.write('\n'.join(poems))

def ds_func(split, shuffle_files=False):
    del shuffle_files
    ds = tf.data.TextLineDataset(os.path.join(DATA_DIR, split+'.txt'))
    ds = ds.shuffle(4096)#.window(3, 1, 1, True).flat_map(lambda x: x.batch(3))
    ds = ds.map(lambda ex: dict(text=ex))
    return ds

for ex in tfds.as_numpy(ds_func("valid").take(5)):
    print(ex['text'])

t5.data.TaskRegistry.remove('poems')
t5.data.TaskRegistry.add(
    "poems",
    # Supply a function which returns a tf.data.Dataset.
    dataset_fn=ds_func,
    splits=["train", "valid"],
    # Supply a function which preprocesses text from the tf.data.Dataset.
    text_preprocessor=[
        lambda sample: t5.data.preprocessors.prefix_lm(sample, label='poem: '),
        # t5.data.preprocessors.lm
    ],
    # Use the same vocabulary that we used for pre-training.
    sentencepiece_model_path=t5.data.DEFAULT_SPM_PATH,
    # Lowercase targets before computing metrics.
    postprocess_fn=t5.data.postprocessors.lower_text, 
    # We'll use accuracy as our evaluation metric.
    metric_fns=[t5.evaluation.metrics.accuracy],
    # Not required, but helps for mixing and auto-caching.
    num_input_examples=counts
)

nq_task = t5.data.TaskRegistry.get("poems")
ds = nq_task.get_dataset(split="valid", sequence_length={"inputs": 512, "targets": 512})

for ex in tfds.as_numpy(ds.take(10)):
    print(ex['inputs'].shape, ex['targets'].shape)

t5.data.MixtureRegistry.remove("all")
t5.data.MixtureRegistry.add(
    "all",
    ["poems", "bbc_news"],
     default_rate=1.0
)

MODEL_SIZE = "3B" #@param["small", "base", "large", "3B", "11B"]
# Public GCS path for T5 pre-trained model checkpoints
BASE_PRETRAINED_DIR = "gs://t5-data/pretrained_models"
PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)
MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE)

if ON_CLOUD and MODEL_SIZE == "3B":
  tf.logging.warn(
      "The `3B` model is too large to use with the 5GB GCS free tier. "
      "Make sure you have at least 25GB on GCS before continuing."
  )
elif ON_CLOUD and MODEL_SIZE == "11B":
  raise ValueError(
      "The `11B` parameter is too large to fine-tune on the `v2-8` TPU "
      "provided by Colab. Please comment out this Error if you're running "
      "on a larger TPU."
  )

# Set parallelism and batch size to fit on v2-8 TPU (if possible).
# Limit number of checkpoints to fit within 5GB (if possible).
model_parallelism, train_batch_size, keep_checkpoint_max = {
    "small": (1, 256, 16),
    "base": (2, 128, 8),
    "large": (8, 64, 1),
    "3B": (8, 16, 1),
    "11B": (8, 16, 1)}[MODEL_SIZE]

tf.io.gfile.makedirs(MODEL_DIR)
# The models from our paper are based on the Mesh Tensorflow Transformer.
model = t5.models.MtfModel(
    model_dir=MODEL_DIR,
    tpu=TPU_ADDRESS,
    tpu_topology=TPU_TOPOLOGY,
    model_parallelism=model_parallelism,
    batch_size=train_batch_size,
    sequence_length={"inputs": 512, "targets": 512},
    learning_rate_schedule=0.003,
    save_checkpoints_steps=1000,
    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,
    iterations_per_loop=100,
)

FINETUNE_STEPS = 150000 #@param {type: "integer"}

model.finetune(
    mixture_or_task_name="all",
    pretrained_model_dir=PRETRAINED_DIR,
    finetune_steps=FINETUNE_STEPS
)

text_1 = "The gods are they who came to earth |NL|And set the seas ablaze with gold." #@param {type:"string"}
text_2 = "\"The New York Movie\", by William Collins|NL|" #@param {type:"string"}
text_3 = "Who alive can say,|NL|\u2018Thou art no Poet\u2014may\u2019st not tell thy dreams?\u2019\u201D|NL|" #@param {type:"string"}
text_4 = "" #@param {type:"string"}

text_list = [text_1, text_2, text_3, text_4]

now = time.time()
# Write out the supplied text list to text files.
predict_inputs_path = os.path.join(MODEL_DIR, "predict_inputs_%d.txt" % now)
predict_outputs_path = os.path.join(MODEL_DIR, "predict_outputs_%d.txt" % now)
# Manually apply preprocessing by prepending "triviaqa question:".
with tf.io.gfile.GFile(predict_inputs_path, "w") as f:
  for q in text_list:
    f.write("poem: %s\n" % q.lower())

# Ignore any logging so that we only see the model's answers to the questions.
with tf_verbosity_level('ERROR'):
  model.batch_size = len(text_list)
  model.predict(
      input_file=predict_inputs_path,
      output_file=predict_outputs_path,
      # Select the most probable output token at each step.
      temperature=1.0,
  )

# The output filename will have the checkpoint appended so we glob to get 
# the latest.
prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + "*"))
print("\nPredictions using checkpoint %s:\n" % prediction_files[-1].split("-")[-1])
with tf.io.gfile.GFile(prediction_files[-1]) as f:
  for q, a in zip(text_list, f):
      print("------Context------\n" + q)
      print("-----Generated-----\n" + a.replace('|NL|', '\n'))
      print()